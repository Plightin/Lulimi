Lulimi: The Zambian Language Engine

Lulimi (Nyanja for "tongue" or "language") is a pioneering Neural Machine Translation (NMT) engine designed to bridge communication gaps between Zambia's four primary local languagesâ€”Nyanja (NY), Bemba (BM), Tonga (TO), and Lozi (LO)â€”and English (EN).

ğŸ¯ Project Goal

To deliver high-quality, real-time machine translation that empowers digital inclusion, governance, and economic growth across Zambia.

âš™ï¸ Architecture: The Pivot Strategy

Due to the low-resource nature of the Zambian languages, the Lulimi engine employs a Pivot Strategy using English as the central intermediary language.

Translation Route

Model Calls

Example (Bemba $\to$ Lozi)

Direct (Zambian $\leftrightarrow$ English)

1 Model Call

BM_to_EN or EN_to_BM

Pivot (Zambian $\leftrightarrow$ Zambian)

2 Model Calls

BM_to_EN then EN_to_LO

ğŸš€ Repository Structure

The repository is organized to separate the community data collection, the core API, and the documentation.

/Lulimi/
â”œâ”€â”€ /backend/                        # Python API and NMT models (Phase 2)
â”‚   â”œâ”€â”€ api.py                       # FastAPI/Flask server and routing logic
â”‚   â”œâ”€â”€ models/                      # Trained NMT model files (large files managed via Git LFS)
â”‚   â””â”€â”€ requirements.txt             # Python dependencies (TensorFlow/PyTorch, FastAPI)
â”‚
â”œâ”€â”€ /frontend/                       # Community Contribution Platform (CCP) UI
â”‚   â””â”€â”€ lulimi_ccp_interface.html    # The main CCP HTML/JS file
â”‚
â”œâ”€â”€ /docs/                           # Project planning and phase documentation
â”‚   â”œâ”€â”€ lulimi_api_structure.md      # API specification
â”‚   â”œâ”€â”€ lulimi_language_engine_proposal.md
â”‚   â””â”€â”€ lulimi_phase1_data.md        # Data Strategy
â”‚
â”œâ”€â”€ README.md                        # This file
â””â”€â”€ .gitignore                       # Standard exclusion list for development


ğŸ› ï¸ Getting Started (Phase 1: Data Collection)

Currently, the most active component is the Community Contribution Platform (CCP), which generates the training data.

Database: Ensure a Firebase Firestore instance is set up to match the public collection paths defined in the CCP code: /artifacts/{appId}/public/data/unvalidated_sentences.

Frontend: The /frontend/lulimi_ccp_interface.html file can be run directly in any browser for immediate community testing and data collection.

ğŸ“ Contribution

We welcome contributions from native speakers, linguists, and developers! Please check the docs/ folder for the full plan and submit issues or pull requests.